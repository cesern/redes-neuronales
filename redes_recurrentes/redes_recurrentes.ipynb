{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "redes_recurrentes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesern/redes-neuronales/blob/master/redes_recurrentes/redes_recurrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uZ0xH7nveILR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/cesern/redes-neuronales/blob/master/redes_recurrentes/imagenes/rn3.png?raw=1\" width=\"200\">\n",
        "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
      ]
    },
    {
      "metadata": {
        "id": "FwcSx8PReILd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
        "\n",
        "# Redes recurrentes, implementación simple\n",
        "\n",
        "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 9 de mayo de 2018.\n",
        "\n",
        "En esta libreta vamos a explorar como desarrollar redes recurrentes, simples y basadas en unidades de memoria de largo y corto plazo (LSTM), aplicadas a la generación automática de texto.\n",
        "\n",
        "Dado que estamos en México en año electoral, y que se vota por diputados y presidentes municipales en muchisimos municipios del país, nos preguntamos si podríamos inventar nuevos municipios para que todos los candidatos tuvieran un lugar que gobernar. Así, generamos una lista con el nombre de todos los municipios de México, y la vamos a usar para aprender los nombres, y generar nombres a partir de una red recurrente. Esto es interesante ya que en México hay muchos municipios cuyos nombres tienen raices del español, el nahuatl, direrentes lenguas mayas, varias lenguas de la familia yuto-azteca, e inclusive algunos que son palabras inventadas (como Mexicali). Así que generar nombres de municipios mexicanos *creibles* es un problema interesante.\n",
        "\n",
        "El archivo con el nombre de los municipios se encuentra para su descarga [aqui](municipios.txt).\n"
      ]
    },
    {
      "metadata": {
        "id": "d7U0TtTQeILt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Redes recurrentes: Desarrollar una red recurrente completamente a pie.\n",
        "\n",
        "Con el fin de entender como funcionan las redes neuronales, vamos a aplicar un modelo de generación de texto *letra a letra*, en el cual tanto la arquitectura como el método de aprendizaje sean programados a mano. \n",
        "\n",
        "No vamos a pedir que lo programen todo solos, simplemente que utilicen el modelo programados en este [gist](https://gist.github.com/karpathy/d4dee566867f8291f086), y lo adapten a leer el nombre de los municipios y a generar nombres de municipios.\n",
        "\n",
        "Para esto:\n",
        "\n",
        "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mñas claro para ti y para mi).\n",
        "\n",
        "2. Copiar y comentar en español el contenido del método de verificción de gradiente (para limpiar el código de errores) y usarlo por unas cuantas iteraciones para demostrar que el algoritmo de entrenamiento funciona correctamente.\n",
        "\n",
        "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
      ]
    },
    {
      "metadata": {
        "id": "lYUycJboeIL2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBh-KH4ifpSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Función de perdida\n",
        "def loss_f(inputs, targets, hprev):\n",
        "    \"\"\"\n",
        "    -imputs y targets arreglo de enteros\n",
        "    -arreglo de Hx1 del estado inicial oculto\n",
        "    -regresa el valor de perdida, gradientes de los parametros y el estado de la ultima capa oculta\n",
        "     \"\"\"\n",
        "    xs, hs, ys, ps = {}, {}, {}, {}\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "    # forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(Whh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcCNmXdMg-Yd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(h, seed_ix, n):\n",
        "  \"\"\" \n",
        "  sample a sequence of integers from the model \n",
        "  h is memory state, seed_ix is seed letter for first time step\n",
        "  \"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AouaBZFhP6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b938c772-c0a3-4380-c5af-62e9ed8a1e96"
      },
      "cell_type": "code",
      "source": [
        "#para usar los archivos de mi drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uj8choNclFpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b35b2ee1-b2b1-41b3-8667-0d9e6d3a11c9"
      },
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/Ultima/redes_recurrentes/municipios.txt\" \"municipios.txt\" \n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  municipios.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NI7_6oYilD8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13702009-3248-412e-9af1-d2005ce79a21"
      },
      "cell_type": "code",
      "source": [
        "# Lectura de datos\n",
        "data = open('municipios.txt', 'r').read() # should be simple plain text file\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# hiperparametros\n",
        "hidden_size = 100 # tamaño de la capa oculta\n",
        "seq_length = 25 # Número de pasos para desenrollar la RNN\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias\n",
        "\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 35530 characters, 68 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FQd8USU0p0OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bfd90586-cfff-4eb2-eb33-2b0117ec84d7"
      },
      "cell_type": "code",
      "source": [
        "_max = 100000\n",
        "while n < _max:\n",
        "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "    if p+seq_length+1 >= len(data) or n == 0: \n",
        "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "        p = 0 # go from start of data\n",
        "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "    \n",
        "    #print('inputs: ', inputs)\n",
        "    #print('targets: ', targets)\n",
        "    \n",
        "    # sample from the model now and then\n",
        "    if n == _max - 1:\n",
        "        sample_ix = sample(hprev, inputs[0], 200)\n",
        "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "        print ('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "    # forward seq_length characters through the net and fetch gradient\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_f(inputs, targets, hprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    if n % (_max/5) == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "  \n",
        "    # perform parameter update with Adagrad\n",
        "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
        "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "    p += seq_length # move data pointer\n",
        "    n += 1 # iteration counter"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 105.487698\n",
            "iter 20000, loss: 41.616021\n",
            "iter 40000, loss: 39.122717\n",
            "iter 60000, loss: 38.468414\n",
            "iter 80000, loss: 37.826173\n",
            "----\n",
            " opolde\n",
            "Jaltán de Mago Bea Villo\n",
            "Mostol\n",
            "La \n",
            "Jaquia\n",
            "Momáus\n",
            "Miutepec de Ló Ho\n",
            "Novús\n",
            "Marva\n",
            "Matarca\n",
            "Miguel Arto\n",
            "Macha Malío Je C. Morrolapen de Lánare Toraco\n",
            "Muguatla\n",
            "Monela\n",
            "Coatelacan\n",
            "Lal Péngo Materde\n",
            "Lu \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hAizzJ_jeIMj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Redes recurrentes tipo LSTM\n",
        "\n",
        "Las redes con unidades LSTM las vimos platicadas en clase, pero no hay nada mejor para entender un tema que implementarlo y comparar los resultados con la red recurrente simple, sin memoria, para esto vamos a hacer lo mismo que antes, pero con unidades LSTM.\n",
        "\n",
        "Para esto vamos a utilizar otro [gist](https://gist.github.com/karpathy/587454dc0146a6ae21fc) del mismo autor (que es una referencia obligada en el tema, por cierto). En este *gist*, el autor presenta un modelo de redes recurrentes LSTM desarrollado con *numpy* incluido el método de entrenamiento, pero no lo aplica a el modelado *letra a letra* como el gist pasado. Para esta parte de la libreta, lo que tienen que realizar es lo siguiente:\n",
        "\n",
        "\n",
        "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mñas claro para ti y para mi).\n",
        "\n",
        "2. Adaptar el modelo propuesto para usarlo en la generación de nombres de municipios.\n",
        "\n",
        "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
      ]
    },
    {
      "metadata": {
        "id": "Xd2vLd04eIMo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Agregar aqui el código, y en cuantas celdas como consideres necesarias\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpZ0ijGPeIMy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Por último, agrega en esta misma celda comentarios sobre la comparación entre los dos modelos vistos, diferencias, similitudes, ventajas, desventajas. Recuerda es una opinión personal basada en tu trabajo, no pongas lo que dice la literatura si no lo que tu experimentaste a la hora de desarrollar y aplicar los modelos, así no concuerde con lo que leas en otros lados."
      ]
    }
  ]
}